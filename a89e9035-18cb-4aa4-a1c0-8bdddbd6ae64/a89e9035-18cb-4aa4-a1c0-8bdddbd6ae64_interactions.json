[
  {
    "method": "analyze_and_create_micros",
    "status": "success",
    "prompt": "WICHTIG: Deine GESAMTE Antwort MUSS ausschlie√ülich ein valides JSON-Objekt sein, das mit { beginnt und mit } endet. KEIN zus√§tzlicher Text, keine Erkl√§rungen, keine Markdown-Formatierungen wie ```json.\n\nDu bist ein Experte f√ºr wissenschaftlich fundiertes Microlearning-Design. Verwandle den folgenden ECHTEN Inhalt in konzentrierte Micro-Lerneinheiten.\n\n**WICHTIG: VERWENDE NUR DEN GEGEBENEN INHALT! ERFINDE NICHTS DAZU!**\n\nüéØ **KERNPRINZIP: EXTREME PR√ÑGNANZ**\n- Jedes Micro: 30-45 Sekunden Lesezeit (ca. 50-75 W√∂rter)\n- Ein Konzept pro Micro\n- Du-Ansprache, konversationell\n- Emojis als visuelle Anker\n\nüß† **STRUKTUR PRO MICRO:**\n1. **Hook** (1 Satz): Interesse wecken\n2. **Kernkonzept** (2-3 S√§tze): Klare Definition aus dem ECHTEN Inhalt\n3. **Praktisches Beispiel** (1-2 S√§tze) - NUR aus dem gegebenen Inhalt!\n\n**AUSGABEFORMAT:**\n{\n  \"micros\": [\n    {\n      \"title\": \"Pr√§gnanter Titel basierend auf echtem Inhalt\",\n      \"knowledge_md\": \"Text basierend auf dem ECHTEN Inhalt unten - NIEMALS erfinden!\",\n      \"visual_title\": \"Visueller Titel\",\n      \"visual_description_text\": \"Handlungsanweisung f√ºr Benutzer\",\n      \"visual_description\": {\n        \"concept\": \"Konzept Name\",\n        \"description\": \"Detaillierte Beschreibung der Visualisierung\",\n        \"interaction_type\": \"click_explore\",\n        \"learning_goal\": \"Lernziel basierend auf echtem Inhalt\",\n        \"fixed_dimensions\": {\"width\": 1024, \"height\": 768}\n      },\n      \"mini_quiz\": {\n        \"question\": \"Quiz-Frage basierend auf dem ECHTEN Inhalt\",\n        \"options\": [\"Option A\", \"Option B\", \"Option C\", \"Option D\"],\n        \"correct_answer\": 1,\n        \"explanation\": \"Erkl√§rung basierend auf echtem Inhalt\"\n      }\n    }\n  ]\n}\n\n**ECHTER INHALT ZU ANALYSIEREN (VERWENDE NUR DIESEN!):**\nIn recent years, Large Language Models (LLMs) have been undergoing rapid iteration and\nevolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap to-\nwards Artificial General Intelligence (AGI). Beyond closed-source models, open-source models,\nincluding DeepSeek series (DeepSeek-AI, 2024a,b,c; Guo et al., 2024), LLaMA series (AI@Meta,\n2024a,b; Touvron et al., 2023a,b), Qwen series (Qwen, 2023, 2024a,b), and Mistral series (Jiang\net al., 2023; Mistral, 2024), are also making significant strides, endeavoring to close the gap with\ntheir closed-source counterparts. To further push the boundaries of open-source model capa-\nbilities, we scale up our models and introduce DeepSeek-V3, a large Mixture-of-Experts (MoE)\nmodel with 671B parameters, of which 37B are activated for each token.\nWith a forward-looking perspective, we consistently strive for strong model performance\nand economical costs. Therefore, in terms of architecture, DeepSeek-V3 still adopts Multi-head\nLatent Attention (MLA) (DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE (Dai\net al., 2024) for cost-effective training. These two architectures have been validated in DeepSeek-\nV2 (DeepSeek-AI, 2024c), demonstrating their capability to maintain robust model performance\nwhile achieving efficient training and inference. Beyond the basic architecture, we implement\ntwo additional strategies to further enhance the model capabilities. Firstly, DeepSeek-V3 pi-\noneers an auxiliary-loss-free strategy (Wang et al., 2024a) for load balancing, with the aim of\nminimizing the adverse impact on model performance that arises from the effort to encourage\nload balancing. Secondly, DeepSeek-V3 employs a multi-token prediction training objective,\nwhich we have observed to enhance the overall performance on evaluation benchmarks.\nIn order to achieve efficient training, we support the FP8 mixed precision training and\nimplement comprehensive optimizations for the training framework. Low-precision training\nhas emerged as a promising solution for efficient training (Dettmers et al., 2022; Kalamkar et al.,\n2019; Narang et al., 2017; Peng et al., 2023b), its evolution being closely tied to advancements in\nhardware capabilities (Luo et al., 2024; Micikevicius et al., 2022; Rouhani et al., 2023a). In this\nwork, we introduce an FP8 mixed precision training framework and, for the first time, validate\nits effectiveness on an extremely large-scale model. Through the support for FP8 computation\nand storage, we achieve both accelerated training and reduced GPU memory usage. As for\nthe training framework, we design the DualPipe algorithm for efficient pipeline parallelism,\nwhich has fewer pipeline bubbles and hides most of the communication during training through\ncomputation-communication overlap. This overlap ensures that, as the model further scales up,\nas long as we maintain a constant computation-to-communication ratio, we can still employ\nfine-grained experts across nodes while achieving a near-zero all-to-all communication overhead.\nIn addition, we also develop efficient cross-node all-to-all communication kernels to fully utilize\nInfiniBand (IB) and NVLink bandwidths. Furthermore, we meticulously optimize the memory\nfootprint, making it possible to train DeepSeek-V3 without using costly tensor parallelism.\nCombining these efforts, we achieve high training efficiency.\nDuring pre-training, we train DeepSeek-V3 on 14.8T high-quality and diverse tokens. The\npre-training process is remarkably stable. Throughout the entire training process, we did not\nencounter any irrecoverable loss spikes or have to roll back. Next, we conduct a two-stage\ncontext length extension for DeepSeek-V3. In the first stage, the maximum context length is\nextended to 32K, and in the second stage, it is further extended to 128K. Following this, we\nconduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL)\non the base model of DeepSeek-V3, to align it with human preferences and further unlock its\npotential. During the post-training stage, we distill the reasoning capability from the DeepSeek-\nR1 series of models, and meanwhile carefully maintain the balance between model accuracy\n4\nTraining Costs Pre-Training Context Extension Post-Training Total\nin H800 GPU Hours 2664K 119K 5K 2788K\nin USD $5.328M $0.238M $0.01M $5.576M\nTable 1|Training costs of DeepSeek-V3, assuming the rental price of H800 is $2 per GPU hour.\nand generation length.\nWe evaluate DeepSeek-V3 on a comprehensive array of benchmarks. Despite its economical\ntraining costs, comprehensive evaluations reveal that DeepSeek-V3-Base has emerged as the\nstrongest open-source base model currently available, especially in code and math. Its chat\nversion also outperforms other open-source models and achieves performance comparable to\nleading closed-source models, including GPT-4o and Claude-3.5-Sonnet, on a series of standard\nand open-ended benchmarks.\nLastly, we emphasize again the economical training costs of DeepSeek-V3, summarized in\nTable 1, achieved through our optimized co-design of algorithms, frameworks, and hardware.\nDuring the pre-training stage, training DeepSeek-V3 on each trillion tokens requires only 180K\nH800 GPU hours, i.e., 3.7 days on our cluster with 2048 H800 GPUs. Consequently, our pre-\ntraining stage is completed in less than two months and costs 2664K GPU hours. Combined\nwith 119K GPU hours for the context length extension and 5K GPU hours for post-training,\nDeepSeek-V3 costs only 2.788M GPU hours for its full training. Assuming the rental price of\nthe H800 GPU is $2 per GPU hour, our total training costs amount to only $5.576M. Note that\nthe aforementioned costs include only the official training of DeepSeek-V3, excluding the costs\nassociated with prior research and ablation experiments on architectures, algorithms, or data.\nOur main contribution includes:\nArchitecture: Innovative Load Balancing Strategy and Training Objective\n‚Ä¢On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free\nstrategy for load balancing, which minimizes the performance degradation that arises\nfrom encouraging load balancing.\n‚Ä¢We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model\nperformance. It can also be used for speculative decoding for inference acceleration.\nPre-Training: Towards Ultimate Training Efficiency\n‚Ä¢We design an FP8 mixed precision training framework and, for the first time, validate the\nfeasibility and effectiveness of FP8 training on an extremely large-scale model.\n‚Ä¢Through the co-design of algorithms, frameworks, and hardware, we overcome the\ncommunication bottleneck in cross-node MoE training, achieving near-full computation-\ncommunication overlap. This significantly enhances our training efficiency and reduces the\ntraining costs, enabling us to further scale up the model size without additional overhead.\n‚Ä¢At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of\nDeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model.\nThe subsequent training stages after pre-training require only 0.1M GPU hours.\nPost-Training: Knowledge Distillation from DeepSeek-R1\n‚Ä¢We introduce an innovative methodology to distill reasoning capabilities from the long-\nChain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models,\ninto standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the\n5\n\n**ANWEISUNG:** Analysiere den obigen ECHTEN Inhalt und erstelle daraus Micro-Lerneinheiten. VERWENDE NUR INFORMATIONEN AUS DIESEM INHALT! Erfinde nichts √ºber KI, Machine Learning oder andere Themen, die nicht im Text stehen!",
    "response": "{\n  \"micros\": [\n    {\n      \"title\": \"DeepSeek-V3: Ein √úberblick\",\n      \"knowledge_md\": \"Hast du dich jemals gefragt, wie fortschrittliche KI-Modelle wie DeepSeek-V3 aufgebaut sind? üí° DeepSeek-V3 ist ein gro√ües Mixture-of-Experts (MoE) Modell mit beeindruckenden 671 Milliarden Parametern. F√ºr jede Anfrage werden davon 37 Milliarden Parameter aktiviert, was eine effiziente Verarbeitung erm√∂glicht. Es ist ein Beispiel f√ºr die Fortschritte bei Open-Source-Modellen. Im Vergleich zu anderen Open-Source-Modellen wie LLaMA oder Qwen, die ebenfalls gro√üe Fortschritte machen, setzt DeepSeek-V3 neue Ma√üst√§be in der Skalierung.\",\n      \"visual_title\": \"DeepSeek-V3: MoE-Modell\",\n      \"visual_description_text\": \"Klicke auf die verschiedenen Bereiche, um mehr √ºber die Parameter und Aktivierung zu erfahren.\",\n      \"visual_description\": {\n        \"concept\": \"DeepSeek-V3 Architektur\",\n        \"description\": \"Eine schematische Darstellung eines MoE-Modells. Zeige einen gro√üen Kreis f√ºr '671B Parameter' und einen kleineren, hervorgehobenen Bereich innerhalb des Kreises f√ºr '37B aktivierte Parameter pro Token'. Pfeile zeigen von 'Input' zu den aktivierten Parametern und dann zu 'Output'.\",\n        \"interaction_type\": \"click_explore\",\n        \"learning_goal\": \"Verstehen, dass DeepSeek-V3 ein gro√ües MoE-Modell ist und wie viele Parameter aktiviert werden.\",\n        \"fixed_dimensions\": {\"width\": 1024, \"height\": 768}\n      },\n      \"mini_quiz\": {\n        \"question\": \"Wie viele Parameter werden in DeepSeek-V3 f√ºr jeden Token aktiviert?\",\n        \"options\": [\"671 Milliarden\", \"37 Milliarden\", \"128 Tausend\", \"14.8 Billionen\"],\n        \"correct_answer\": 1,\n        \"explanation\": \"DeepSeek-V3 hat 671 Milliarden Parameter, aber nur 37 Milliarden werden f√ºr jeden Token aktiviert.\"\n      }\n    },\n    {\n      \"title\": \"Effiziente Architektur: MLA & DeepSeekMoE\",\n      \"knowledge_md\": \"M√∂chtest du wissen, wie DeepSeek-V3 trotz seiner Gr√∂√üe so effizient arbeiten kann? üöÄ DeepSeek-V3 nutzt die Multi-head Latent Attention (MLA) f√ºr effiziente Inferenz und DeepSeekMoE f√ºr kosteng√ºnstiges Training. Diese Architekturen wurden bereits in DeepSeek-V2 validiert. Sie erm√∂glichen robuste Modellleistung bei hoher Effizienz. Dank MLA und DeepSeekMoE kann das Modell seine Leistung beibehalten, w√§hrend es gleichzeitig effizient trainiert und inferiert wird, √§hnlich wie es schon bei DeepSeek-V2 der Fall war.\",\n      \"visual_title\": \"MLA & DeepSeekMoE\",\n      \"visual_description_text\": \"Klicke auf die Architekturen, um ihre Vorteile zu sehen.\",\n      \"visual_description\": {\n        \"concept\": \"DeepSeek-V3 Effizienz\",\n        \"description\": \"Zwei nebeneinander liegende Boxen: Eine beschriftet mit 'Multi-head Latent Attention (MLA)' und darunter 'Effiziente Inferenz'. Die andere Box 'DeepSeekMoE' und darunter 'Kosteng√ºnstiges Training'. Eine dritte Box darunter zeigt 'Validiert in DeepSeek-V2'.\",\n        \"interaction_type\": \"click_explore\",\n        \"learning_goal\": \"Die Rolle von MLA und DeepSeekMoE f√ºr die Effizienz von DeepSeek-V3 verstehen.\",\n        \"fixed_dimensions\": {\"width\": 1024, \"height\": 768}\n      },\n      \"mini_quiz\": {\n        \"question\": \"Welche Architektur tr√§gt in DeepSeek-V3 zu kosteng√ºnstigem Training bei?\",\n        \"options\": [\"Multi-head Latent Attention (MLA)\", \"DeepSeekMoE\", \"FP8 Mixed Precision\", \"DualPipe Algorithmus\"],\n        \"correct_answer\": 1,\n        \"explanation\": \"DeepSeekMoE ist f√ºr kosteng√ºnstiges Training verantwortlich, w√§hrend MLA die effiziente Inferenz unterst√ºtzt.\"\n      }\n    },\n    {\n      \"title\": \"Innovative Trainingsstrategien\",\n      \"knowledge_md\": \"Wie verbessert man die F√§higkeiten eines riesigen KI-Modells noch weiter? ‚ú® DeepSeek-V3 setzt auf zwei zus√§tzliche Strategien: eine neuartige hilfsverlustfreie Strategie f√ºr den Lastausgleich, die Leistungseinbu√üen minimiert. Zudem wird ein Multi-Token Prediction (MTP) Trainingsziel verwendet, das die Gesamtleistung verbessert. Die hilfsverlustfreie Strategie sorgt daf√ºr, dass der Lastausgleich die Modellleistung nicht negativ beeinflusst, w√§hrend MTP die Ergebnisse auf Bewertungs-Benchmarks steigert.\",\n      \"visual_title\": \"Strategien zur Leistungssteigerung\",\n      \"visual_description_text\": \"Entdecke, wie DeepSeek-V3 seine F√§higkeiten erweitert.\",\n      \"visual_description\": {\n        \"concept\": \"DeepSeek-V3 Verbesserungsstrategien\",\n        \"description\": \"Zwei Symbole: Ein Waage-Symbol mit einem Haken f√ºr 'Hilfsverlustfreie Strategie f√ºr Lastausgleich' und darunter 'Minimiert Leistungseinbu√üen'. Daneben ein Text-Symbol mit einem Pfeil f√ºr 'Multi-Token Prediction (MTP) Trainingsziel' und darunter 'Verbessert Gesamtleistung'.\",\n        \"interaction_type\": \"click_explore\",\n        \"learning_goal\": \"Die zwei innovativen Strategien zur Leistungssteigerung von DeepSeek-V3 kennenlernen.\",\n        \"fixed_dimensions\": {\"width\": 1024, \"height\": 768}\n      },\n      \"mini_quiz\": {\n        \"question\": \"Welches Trainingsziel wird in DeepSeek-V3 verwendet, um die Gesamtleistung zu verbessern?\",\n        \"options\": [\"Auxiliary-Loss-Strategy\", \"Single-Token Prediction\", \"Multi-Token Prediction (MTP)\", \"Load Balancing Objective\"],\n        \"correct_answer\": 2,\n        \"explanation\": \"DeepSeek-V3 verwendet ein Multi-Token Prediction (MTP) Trainingsziel, um die Gesamtleistung auf Bewertungs-Benchmarks zu steigern.\"\n      }\n    },\n    {\n      \"title\": \"FP8 Mixed Precision Training\",\n      \"knowledge_md\": \"Kann man das Training riesiger KI-Modelle noch schneller und speichereffizienter machen? ‚ö° DeepSeek-V3 implementiert FP8 Mixed Precision Training, eine vielversprechende L√∂sung f√ºr effizientes Training. Diese Technik wurde erstmals auf einem extrem gro√üen Modell validiert. Sie erm√∂glicht beschleunigtes Training und reduziert den GPU-Speicherverbrauch. Durch die Unterst√ºtzung von FP8-Berechnungen und -Speicherung kann DeepSeek-V3 schneller trainiert werden und ben√∂tigt weniger GPU-Speicher, was die Effizienz erheblich steigert.\",\n      \"visual_title\": \"FP8 Mixed Precision\",\n      \"visual_description_text\": \"Klicke, um die Vorteile von FP8 zu sehen.\",\n      \"visual_description\": {\n        \"concept\": \"FP8 Training\",\n        \"description\": \"Ein Diagramm, das zwei S√§ulen zeigt: Eine S√§ule 'Ohne FP8' (h√∂herer Speicherverbrauch, langsameres Training) und eine S√§ule 'Mit FP8' (niedrigerer Speicherverbrauch, schnelleres Training). Symbole f√ºr GPU und Speicher.\",\n        \"interaction_type\": \"click_explore\",\n        \"learning_goal\": \"Die Vorteile von FP8 Mixed Precision Training f√ºr DeepSeek-V3 verstehen.\",\n        \"fixed_dimensions\": {\"width\": 1024, \"height\": 768}\n      },\n      \"mini_quiz\": {\n        \"question\": \"Was sind die Hauptvorteile des FP8 Mixed Precision Trainings in DeepSeek-V3?\",\n        \"options\": [\"Erh√∂hte Modellgenauigkeit und Stabilit√§t\", \"Beschleunigtes Training und reduzierter GPU-Speicherverbrauch\", \"Verbesserter Lastausgleich und geringere Kosten\", \"L√§ngere Kontextl√§ngen und bessere menschliche Ausrichtung\"],\n        \"correct_answer\": 1,\n        \"explanation\": \"FP8 Mixed Precision Training erm√∂glicht beschleunigtes Training und reduziert den GPU-Speicherverbrauch.\"\n      }\n    },\n    {\n      \"title\": \"Beeindruckende Trainingskosten & -effizienz\",\n      \"knowledge_md\": \"Was kostet es, ein Top-KI-Modell wie DeepSeek-V3 zu trainieren? Die Zahlen werden dich √ºberraschen! üí∞ DeepSeek-V3 wurde auf 14.8 Billionen hochwertigen Tokens vortrainiert. Die gesamten Trainingskosten, inklusive Vortraining, Kontextl√§ngenerweiterung und Nachtraining, belaufen sich auf nur 2.788 Millionen H800 GPU-Stunden. Dies entspricht etwa 5.576 Millionen US-Dollar. Das Vortraining allein dauerte weniger als zwei Monate und kostete 2.664 Millionen GPU-Stunden, was die enorme Effizienz des Designs unterstreicht.\",\n      \"visual_title\": \"DeepSeek-V3 Trainingskosten\",\n      \"visual_description_text\": \"Erkunde die Aufschl√ºsselung der Trainingskosten.\",\n      \"visual_description\": {\n        \"concept\": \"Trainingskosten DeepSeek-V3\",\n        \"description\": \"Ein Kreisdiagramm oder Balkendiagramm, das die Aufteilung der GPU-Stunden zeigt: Vortraining (2664K), Kontextl√§ngenerweiterung (119K), Nachtraining (5K). Gesamt: 2788K GPU-Stunden und $5.576M.\",\n        \"interaction_type\": \"click_explore\",\n        \"learning_goal\": \"Die Gesamtkosten und die Verteilung der Trainingsphasen von DeepSeek-V3 verstehen.\",\n        \"fixed_dimensions\": {\"width\": 1024, \"height\": 768}\n      },\n      \"mini_quiz\": {\n        \"question\": \"Wie hoch waren die gesch√§tzten Gesamtkosten f√ºr das Training von DeepSeek-V3 in US-Dollar?\",\n        \"options\": [\"$2.664 Millionen\", \"$0.238 Millionen\", \"$5.576 Millionen\", \"$14.8 Millionen\"],\n        \"correct_answer\": 2,\n        \"explanation\": \"Die Gesamtkosten f√ºr das Training von DeepSeek-V3 beliefen sich auf 5.576 Millionen US-Dollar.\"\n      }\n    }\n  ]\n}"
  },
  {
    "timestamp": 1749474168.3248024,
    "type": "prepared_visual_render",
    "visual_type": "journey",
    "mermaid_code_length": 30,
    "success": true,
    "attempts": 1
  },
  {
    "timestamp": 1749474176.9755416,
    "method": "mermaid_correction",
    "model": "gemini-2.5-flash-preview-05-20",
    "prompt_length": 1319,
    "response_length": 175,
    "success": true,
    "original_error": "Kroki returned no data without raising exception"
  },
  {
    "timestamp": 1749474177.6521807,
    "type": "prepared_visual_render",
    "visual_type": "flowchart",
    "mermaid_code_length": 206,
    "success": false,
    "error": "KrokiError(silent_failure): Kroki returned no data without raising exception",
    "attempts": 2,
    "corrected": true
  },
  {
    "timestamp": 1749474188.7949715,
    "type": "prepared_visual_render",
    "visual_type": "flowchart",
    "mermaid_code_length": 195,
    "success": false,
    "error": "LLM correction failed",
    "attempts": 2
  },
  {
    "timestamp": 1749474196.7648287,
    "method": "mermaid_correction",
    "model": "gemini-2.5-flash-preview-05-20",
    "prompt_length": 1306,
    "response_length": 163,
    "success": true,
    "original_error": "Kroki returned no data without raising exception"
  },
  {
    "timestamp": 1749474197.7799654,
    "type": "prepared_visual_render",
    "visual_type": "flowchart",
    "mermaid_code_length": 193,
    "success": false,
    "error": "KrokiError(silent_failure): Kroki returned no data without raising exception",
    "attempts": 2,
    "corrected": true
  },
  {
    "timestamp": 1749474207.0872266,
    "method": "mermaid_correction",
    "model": "gemini-2.5-flash-preview-05-20",
    "prompt_length": 1321,
    "response_length": 174,
    "success": true,
    "original_error": "Kroki returned no data without raising exception"
  },
  {
    "timestamp": 1749474207.8625457,
    "type": "prepared_visual_render",
    "visual_type": "flowchart",
    "mermaid_code_length": 208,
    "success": false,
    "error": "KrokiError(silent_failure): Kroki returned no data without raising exception",
    "attempts": 2,
    "corrected": true
  }
]