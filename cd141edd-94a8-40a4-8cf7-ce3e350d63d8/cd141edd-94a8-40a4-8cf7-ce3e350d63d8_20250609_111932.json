{
  "knowledge_extraction": {
    "format_version": "2.0",
    "micros": [
      {
        "id": "micro_1",
        "title": "DeepSeek-V3: Ein MoE-Modell im √úberblick",
        "knowledge_md": "Hast du dich je gefragt, wie Open-Source LLMs zu den Spitzenmodellen aufschlie√üen? DeepSeek-V3 ist ein gro√ües Mixture-of-Experts (MoE) Modell mit 671 Milliarden Parametern, das den Fortschritt von Open-Source-Modellen vorantreibt. Es nutzt Multi-head Latent Attention (MLA) f√ºr effiziente Inferenz und DeepSeekMoE f√ºr kosteng√ºnstiges Training. Diese Architekturen wurden bereits in DeepSeek-V2 erfolgreich eingesetzt, um starke Leistung bei effizientem Training und Inferenz zu gew√§hrleisten.",
        "visual_title": "DeepSeek-V3: Ein √úberblick",
        "visual_description_text": "Erkunde die Schl√ºsselkomponenten von DeepSeek-V3.",
        "visual_description": {
          "concept": "DeepSeek-V3 Architektur",
          "description": "Eine schematische Darstellung von DeepSeek-V3, die seine MoE-Struktur, Multi-head Latent Attention (MLA) und DeepSeekMoE hervorhebt. Zeige die Parameteranzahl (671B gesamt, 37B aktiviert pro Token).",
          "interaction_type": "click_explore",
          "learning_goal": "Die grundlegende Architektur und Schl√ºsselkomponenten von DeepSeek-V3 verstehen.",
          "fixed_dimensions": {
            "width": 1024,
            "height": 768
          }
        },
        "visual_specification": {
          "mermaid_type": "graph",
          "structure": {
            "nodes": [
              {
                "id": "center",
                "label": "DeepSeek-V3: Ein MoE-Modell im √úberblick"
              },
              {
                "id": "aspect1",
                "label": "Aspekt 1"
              },
              {
                "id": "aspect2",
                "label": "Aspekt 2"
              },
              {
                "id": "aspect3",
                "label": "Aspekt 3"
              }
            ],
            "edges": [
              {
                "from": "center",
                "to": "aspect1"
              },
              {
                "from": "center",
                "to": "aspect2"
              },
              {
                "from": "center",
                "to": "aspect3"
              }
            ]
          },
          "styling": {
            "theme": "dark",
            "primary_color": "#7F5AF0",
            "secondary_color": "#2CB67D",
            "accent_color": "#FBB040",
            "background_color": "#16213E",
            "text_color": "#E4E6EA"
          },
          "interactivity": {
            "type": "click_explore",
            "hover_effects": true,
            "click_actions": true,
            "animations": true
          },
          "dimensions": {
            "width": 1024,
            "height": 768
          },
          "ready_to_render": true,
          "fallback_description": "Eine schematische Darstellung von DeepSeek-V3, die seine MoE-Struktur, Multi-head Latent Attention (MLA) und DeepSeekMoE hervorhebt. Zeige die Parameteranzahl (671B gesamt, 37B aktiviert pro Token)."
        },
        "has_mini_quiz": true,
        "mini_quiz": {
          "question": "Welche Architekturmerkmale von DeepSeek-V3 sind f√ºr effiziente Inferenz und kosteng√ºnstiges Training verantwortlich?",
          "options": [
            "Reinforcement Learning und SFT",
            "Multi-head Latent Attention (MLA) und DeepSeekMoE",
            "FP8 Mixed Precision Training und DualPipe",
            "Auxiliary-loss-free strategy und Multi-token prediction"
          ],
          "correct_answer": 1,
          "explanation": "DeepSeek-V3 nutzt Multi-head Latent Attention (MLA) f√ºr effiziente Inferenz und DeepSeekMoE f√ºr kosteng√ºnstiges Training, beides bew√§hrte Architekturen aus DeepSeek-V2."
        }
      },
      {
        "id": "micro_2",
        "title": "Effizientes Training: FP8 und DualPipe",
        "knowledge_md": "Wie kann ein riesiges Modell wie DeepSeek-V3 so effizient trainiert werden? üöÄ DeepSeek-V3 nutzt FP8 Mixed Precision Training, um die Trainingsgeschwindigkeit zu erh√∂hen und GPU-Speicher zu sparen. Der DualPipe-Algorithmus verbirgt zudem Kommunikation durch √úberlappung mit Berechnungen, was Pipeline-Bubbles minimiert. Dies erm√∂glicht es, feingranulare Experten √ºber Knoten hinweg zu nutzen, mit nahezu null Kommunikations-Overhead, selbst bei weiterer Skalierung.",
        "visual_title": "Effizientes Training: FP8 & DualPipe",
        "visual_description_text": "Visualisiere die Vorteile von FP8 und DualPipe.",
        "visual_description": {
          "concept": "FP8 und DualPipe",
          "description": "Eine Animation, die zeigt, wie FP8 Mixed Precision Training den Speicherverbrauch reduziert und die Geschwindigkeit erh√∂ht. Daneben eine Darstellung des DualPipe-Algorithmus, der zeigt, wie Kommunikationszeiten durch √úberlappung mit Berechnungen minimiert werden, um 'Pipeline Bubbles' zu vermeiden.",
          "interaction_type": "click_explore",
          "learning_goal": "Verstehen, wie FP8 Mixed Precision Training und der DualPipe-Algorithmus die Trainingseffizienz von DeepSeek-V3 verbessern.",
          "fixed_dimensions": {
            "width": 1024,
            "height": 768
          }
        },
        "visual_specification": {
          "mermaid_type": "timeline",
          "structure": {
            "timeline": {
              "title": "Effizientes Training: FP8 und DualPipe",
              "sections": [
                {
                  "period": "Phase 1",
                  "events": [
                    "Einf√ºhrung",
                    "Grundlagen"
                  ]
                },
                {
                  "period": "Phase 2",
                  "events": [
                    "Vertiefung",
                    "Anwendung"
                  ]
                },
                {
                  "period": "Phase 3",
                  "events": [
                    "Meisterschaft",
                    "Integration"
                  ]
                }
              ]
            }
          },
          "styling": {
            "theme": "dark",
            "primary_color": "#7F5AF0",
            "secondary_color": "#2CB67D",
            "accent_color": "#FBB040",
            "background_color": "#16213E",
            "text_color": "#E4E6EA"
          },
          "interactivity": {
            "type": "click_explore",
            "hover_effects": true,
            "click_actions": true,
            "animations": true
          },
          "dimensions": {
            "width": 1024,
            "height": 768
          },
          "ready_to_render": true,
          "fallback_description": "Eine Animation, die zeigt, wie FP8 Mixed Precision Training den Speicherverbrauch reduziert und die Geschwindigkeit erh√∂ht. Daneben eine Darstellung des DualPipe-Algorithmus, der zeigt, wie Kommunikationszeiten durch √úberlappung mit Berechnungen minimiert werden, um 'Pipeline Bubbles' zu vermeiden."
        },
        "has_mini_quiz": true,
        "mini_quiz": {
          "question": "Welche Technologie hilft DeepSeek-V3, den GPU-Speicherverbrauch zu senken und das Training zu beschleunigen?",
          "options": [
            "Multi-head Latent Attention (MLA)",
            "Reinforcement Learning (RL)",
            "FP8 Mixed Precision Training",
            "Auxiliary-loss-free strategy"
          ],
          "correct_answer": 2,
          "explanation": "FP8 Mixed Precision Training erm√∂glicht es DeepSeek-V3, den GPU-Speicherverbrauch zu senken und das Training zu beschleunigen."
        }
      },
      {
        "id": "micro_3",
        "title": "DeepSeek-V3: Trainingsphasen und Kosten",
        "knowledge_md": "Wie entsteht ein leistungsstarkes LLM wie DeepSeek-V3 und was kostet das? üí∏ DeepSeek-V3 wurde stabil auf 14.8 Billionen Tokens vortrainiert. Danach erfolgte eine zweistufige Kontextl√§ngenerweiterung auf bis zu 128K. Post-Training, mit SFT und RL, passte das Modell an menschliche Pr√§ferenzen an. Die Gesamtkosten betrugen 2.788 Millionen H800 GPU-Stunden, rund $5.576 Millionen USD.",
        "visual_title": "DeepSeek-V3: Trainingsphasen & Kosten",
        "visual_description_text": "Verfolge den Trainingsprozess und die Kosten von DeepSeek-V3.",
        "visual_description": {
          "concept": "Trainingsphasen und Kosten",
          "description": "Ein Zeitstrahl, der die drei Hauptphasen des DeepSeek-V3-Trainings darstellt: Vortraining (14.8T Tokens), Kontextl√§ngenerweiterung (bis 128K) und Post-Training (SFT & RL). Zeige die GPU-Stunden und die gesch√§tzten Kosten f√ºr jede Phase und die Gesamtsumme ($5.576M).",
          "interaction_type": "click_explore",
          "learning_goal": "Die verschiedenen Trainingsphasen und die damit verbundenen Kosten von DeepSeek-V3 verstehen.",
          "fixed_dimensions": {
            "width": 1024,
            "height": 768
          }
        },
        "visual_specification": {
          "mermaid_type": "timeline",
          "structure": {
            "timeline": {
              "title": "DeepSeek-V3: Trainingsphasen und Kosten",
              "sections": [
                {
                  "period": "Phase 1",
                  "events": [
                    "Einf√ºhrung",
                    "Grundlagen"
                  ]
                },
                {
                  "period": "Phase 2",
                  "events": [
                    "Vertiefung",
                    "Anwendung"
                  ]
                },
                {
                  "period": "Phase 3",
                  "events": [
                    "Meisterschaft",
                    "Integration"
                  ]
                }
              ]
            }
          },
          "styling": {
            "theme": "dark",
            "primary_color": "#7F5AF0",
            "secondary_color": "#2CB67D",
            "accent_color": "#FBB040",
            "background_color": "#16213E",
            "text_color": "#E4E6EA"
          },
          "interactivity": {
            "type": "click_explore",
            "hover_effects": true,
            "click_actions": true,
            "animations": true
          },
          "dimensions": {
            "width": 1024,
            "height": 768
          },
          "ready_to_render": true,
          "fallback_description": "Ein Zeitstrahl, der die drei Hauptphasen des DeepSeek-V3-Trainings darstellt: Vortraining (14.8T Tokens), Kontextl√§ngenerweiterung (bis 128K) und Post-Training (SFT & RL). Zeige die GPU-Stunden und die gesch√§tzten Kosten f√ºr jede Phase und die Gesamtsumme ($5.576M)."
        },
        "has_mini_quiz": true,
        "mini_quiz": {
          "question": "Wie hoch waren die gesch√§tzten Gesamtkosten f√ºr das Training von DeepSeek-V3?",
          "options": [
            "Unter $1 Million USD",
            "Zwischen $1 Million und $3 Millionen USD",
            "√úber $5 Millionen USD",
            "Die Kosten wurden nicht angegeben"
          ],
          "correct_answer": 2,
          "explanation": "Die Gesamtkosten f√ºr das Training von DeepSeek-V3 beliefen sich auf etwa $5.576 Millionen USD."
        }
      }
    ],
    "metadata": {
      "original_pdf_chapters_count": 1,
      "total_micros_generated": 3
    }
  },
  "metadata": {
    "source_file": "temp\\cd141edd-94a8-40a4-8cf7-ce3e350d63d8.pdf",
    "generation_date": "2025-06-09T11:19:32.142411",
    "source_filename": "cd141edd-94a8-40a4-8cf7-ce3e350d63d8"
  }
}